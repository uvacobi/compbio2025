---
layout: aakrosh_markdown
title: "Hidden Markov Models (HMMs)"
---

## {{page.title}}
![hmm](images/hmm/hiddenmarkov.svg)

Note: HMMs are well-known for their effectiveness in modeling the correlations between adjacent symbols, domains, or events, and they have been extensively used in various fields, especially in speech recognition and digital communication. Considering the remarkable success of HMMs in engineering, it is no surprise that a wide range of problems in biological sequence analysis have also benefited from them. 


---

## What do you remember about dynamic programming?

What is dynamic programming really?


- introduced in 1950s by Richard Bellman of Rand Corp <!-- .element : class="fragment" data-fragment-index="1" --> <!-- .element : class="fragment" data-fragment-index="1" -->
- it's hard to define  <!-- .element : class="fragment" data-fragment-index="1" --> 
- it's extremely broad <!-- .element : class="fragment" data-fragment-index="1" --> 
- Paper: [The theory of dynamic programming](https://www.ams.org/journals/bull/1954-60-06/S0002-9904-1954-09848-8/)  <!-- .element : class="fragment" data-fragment-index="1" --> 

Note:
- give a chance to recall what the class remembers from earlier lectures about DP.
- the point of this is that it's a bit hard to define. So, we'll go over it again now, from this historical perspective with excerpts from the paper.
---

## The setting

![example](images/hmm/dynamic-programming-1.png)

---

## The insight:

The principle of **Local Decision Sufficiency**

![example](images/hmm/dynamic-programming-2.png)

<span class="fragment">The Markov property. The essence of dynamic programming is "Do the best you can from where you are." <span class="fragment">But wait...</span> 

<span class="fragment">

![example](images/hmm/dynamic-programming-future.png)

</span>

Notes:
- Q: does there seem to be a contradiction between these statements?
- The "problems of the future" -- To determine what's the optimal choice now, you need to know how things would turn in each of your possible decisions, right?
- So, let's revise the statement slightly.


---

## The insight:

The principle of **Future-Aware Decision Making**

The essence of dynamic programming is "Do the best you can from where you are...assuming your future decisions will be optimal."

![example](images/hmm/dynamic-programming-3.png)<!-- .element : class="fragment" data-fragment-index="1" -->

Principle of Optimality — all you need is your current position and the optimal strategy from that point forward.<!-- .element : class="fragment" data-fragment-index="1" -->

---

<img src="images/hmm/dynamic-programming-path.png" width="400px"/>

It’s like hiking the shortest path to the top of a mountain: no matter where you are on the path, the route from that point to the summit must also be the shortest — otherwise, your whole path wouldn't be the shortest overall.

---

Consider a process determined at any time by an M-dimensional vector

$$
\mathbf{p} = (x_0, x_1, x_2, ..., x_m)
$$

Consider a set of transformations $\mathbf{T} = { T_k }$,  
which are functions that transform $\mathbf{p}$  

$$
T_k(p) = p'
$$

We want to maximize our "return" -- the output of some scalar function $R(p)$ of the final state.

---

We want to select a series of transformations,  
called "policy" $P=(T_1, T_2, ... T_N)$,  
which will yield successive states:

$$
p_1 = T_1(p_0)  \\\\
p_2 = T_2(p_1)  \\\\
...... \\\\
p_N = T_N(p_{N-1})
$$

The maximum value of $R(P_N)$, determined by an optimal policy, will only be a function of the initial vector $p_0$ and the number of stages N. The optimal return value is: 

$$ 
f_N(p) = Max_{P}R(p_N)
$$



---

$$ 
f_N(p) = Max_{P}R(p_N)
$$

Choose our first transformation $T_1(p_0)$.  
The maximum return from the following (N-1) stages is by definition:

$$
f_{N-1}(T_1(p_0)) \\\\
= f_{N-1}(p_1)
$$




Thus:

$$
f_N(p) = Max_{P}R(p_N) = Max_{k} f_{N-1}(T_k(p))
$$

Given the optimal policy, $R(P_N)$ is a function of the initial vector $p_0$ and the number of stages N.

---

## Dynamic programming: the gist

You have a sequence of states, and at each time step, you make a decision to try to achieve the best final outcome.

The brute-force method is to examine all possible decision sequences, but this is infeasible for long horizons. <!-- .element : class="fragment" --> 

Instead, dynamic programming divides into sub-problems: at any point, the optimal decision needs only the current state.  Assuming all future decisions follow the optimal policy, the optimal decision at the current state is the one that leads to the state from which the optimal total outcome can be achieved. <!-- .element : class="fragment" --> 

This concept can apply to a huge array of problems, from investing to aircraft control to...computational biology. <!-- .element : class="fragment" --> 

Note:
1. Optimal Substructure

The optimal solution to the overall problem contains within it optimal solutions to subproblems. You can build the full solution by combining optimal solutions to smaller parts.Example: the shortest path from A to C via B must include the shortest path from A to B.

2. Overlapping Subproblems

The same subproblems are solved multiple times across different branches of the decision process. DP avoids redundant work by storing solutions.

---

## A toy HMM for 5′ splice site recognition

![example](images/hmm/example.jpg.webp)

<small>From: [What is a hidden Markov model?](https://www.nature.com/articles/nbt1004-1315)  
Don't confuse a state-space diagram with a [graphical model diagram](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-018-0629-0/figures/2)
</small>

Note: Often, biological sequence analysis is just a matter of putting the right label on each residue. In gene identification, we want to label nucleotides as exons, introns, or intergenic sequence. In sequence alignment, we want to associate residues in a query sequence with homologous residues in a target database sequence. We can always write an ad hoc program for any given problem, but the same frustrating issues will always recur. One is that we want to incorporate heterogeneous sources of information. A genefinder, for instance, ought to combine splice-site consensus, codon bias, exon/ intron length preferences and open reading frame analysis into one scoring system. How should these parameters be set? How should different kinds of information be weighted? A second issue is to interpret results probabilistically. Finding a best scoring answer is one thing, but what does the score mean, and how confident are we that the best scoring answer is correct? A third issue is extensibility. The moment we perfect our ad hoc genefinder, we wish we had also modeled translational initiation consensus, alternative splicing and a polyadenylation signal. Too often, piling more reality onto a fragile ad hoc program makes it collapse under its own weight.

---

## Hidden Markov Models (HMMs)
* Provide a foundation for probabilistic models of linear sequence ‘labeling’ problems
* Can be designed just by drawing a graph diagram
* Originally developed for applications to voice recognition
* Applications include: Gene prediction, protein secondary structure prediction, copy-number variation, chromatin-state assignment, chromatin topology ...

Note: Joint probability $P(A,B) = P(A|B)  P(B) = P(B|A)  P(A)$, Marginal probability $P(X=A) = \sum_y P(X=A, Y=y_i)$

---

## Markov chains 

## &darr; 

## Hidden Markov models

---

## Markov models
* Set of states: $S \in {s_1, s_2, \ldots, s_n}$
* Process moves from one state to another generating a sequence of $L$ states: $x_{1}, x_{2}, \ldots, x_{L}, $
* Markov property:  The probability of a symbol depends only on the preceding symbol, not the entire previous sequence  
  $$P(x_{L} = s|x_{1},x_{2},\ldots,x_{i(L-1)}) = P(x_{L}=s|x_{i(L-1)})$$

* A Markov chain is a simple markov model defined by:
  * transition probabilities $a_{st} = P(x_i=t| x_{i-1}=s)$, $A=\\{a_{ij}\\}$
  * initial probabilities: $a_{0s} = P(x_1=s)$

Note:
- A Markov model is a general probabilistic model where the future state of a system depends only on the present state, not on the past. A Markov chain is a specific type of Markov model where the states are discrete and the time is also discrete. In essence, a Markov chain is a simple, discrete-time Markov model.

---

## Example of a Markov chain
![markov](images/hmm/markov.svg) <!-- .element width="80%" height="80%" -->

$A = \begin{bmatrix}
0.7 & 0.3\\\\\\
0.4 & 0.6
\end{bmatrix}$,
$a_{0s} = (0.4, 0.6)$

Note: So let's consider I have two coins, one of them is fair and the other one is loaded. 

---

## Calculation of sequence probability


Multiplication Rule of probability:

$$
P(A, B) = P(A|B) P(B)
$$

$$
\begin{eqnarray}
P(x_{1}, \ldots, x_{L}) &=& P(x_{L} | x_{1}, \ldots, x_{(kL-1)}) P(x_{1}, \ldots, x_{(L-1)})\\\\\\
\end{eqnarray}
$$

By Markov property, the probability of a state sequence is:

$$
\begin{eqnarray}
&=& P(x_{L} | x_{(L-1)}) P(x_{1}, x_{2}, \ldots, x_{(L-1)})\\\\\\
&=& \ldots\\\\\\
&=& P(x_{L} | x_{(L-1)}) \ldots P(x_{2} | x_{1}) P(x_{1}) 
\end{eqnarray}
$$

---

## Calculation of sequence probability
![markov](images/hmm/markov.svg) <!-- .element width="60%" height="60%" -->

$a_0 = (0.4, 0.6)$

Suppose we want to calculate $P(L,L,F,F)$

$$\begin{eqnarray}
P(L,L,F,F) &=& P(F|L,L,F) P(L,L,F) \\\\\\
&=& P(F|F) P(F|L,L) P(L,L)\\\\\\
&=& P(F|F) P(F|L) P(L|L) P(L)\\\\\\
&=& 0.7 \times 0.4 \times 0.6 \times 0.6
\end{eqnarray}$$

---

## Hidden Markov Models
* Set of states: $S \in {s_1, s_2, \ldots, s_n}$
* Process moves from one state to another generating a sequence of $L$  states: $x_{1}, x_{2}, \ldots, x_{L}$
* Recall the Markov property: $$P(x_{k}|x_{1},x_{2},\ldots,x_{(L-1)}) = P(x_{L}|x_{(L-1)})$$
* States are not visible, but each state randomly generates one of $L$ observations (or emissions): ${o_1, o_2, \ldots, o_L}$ <!-- .element : class="fragment" data-fragment-index="1" -->

---

## Components of Hidden Markov Models

The following need to be defined for Model $M = (A, B, a_0)$:

* transition probabilities:  
   $ \mathbf{A} = \\{ a_{ij} \\} $  
   $a_{st} = P(x_i=t |x_{i-1} =s)$
* observation/emission probabilities:  
  $\mathbf{B} = \\{ e_k(b) \\}$  
  $e_k(b) = P(x_i=b | \pi_i=k)$  
  (probability of seeing symbol $b$ when in state $k$)
* initial probabilities:  
  $a_{0s} = P(x_1 = s)$

<!-- ---

## Components of an HMM

- transition from state $k$ to state $l$: $\mathbf{A} = {a_{kl}}$
  - initiation probabilities: $a_{0k}$, or $a_0$
- emission probabilities: $\mathbf{B} = {e_k}$
 -->

---

## Example 
![hidden_markov](images/hmm/hiddenmarkov.svg) <!-- .element width="60%" height="60%" -->

$A = \begin{bmatrix}
0.7 & 0.3\\\\\\
0.4 & 0.6
\end{bmatrix}$,
$B = \begin{bmatrix}
0.5 & 0.5\\\\\\
0.3 & 0.7
\end{bmatrix}$,
$a_{0} = (0.4, 0.6)$

---

## Calculation of sequence probability
Suppose we want to calculate $P( \\{ H,H \\} )$

$$\begin{eqnarray}
P( \\{ H,H \\} ) &=& P( \\{ H,H \\}, \\{ F,F \\}) + \\\\\\
& & P( \\{ H,H \\}, \\{ F,L \\}) +\\\\\\
& & P( \\{ H,H \\}, \\{ L,F \\}) +\\\\\\
& & P( \\{ H,H \\}, \\{ L,L \\})
\end{eqnarray}$$

Where: 

$$\begin{eqnarray}
P( \\{ H,H \\}, \\{ F,F \\}) &=& P( \\{ H,H \\} | \\{ F,F \\}) P(\\{ F,F \\})\\\\\\
&=& P(H|F) P(H|F) P(F|F) P(F)
\end{eqnarray}$$

Note:
- Just sum over all possible hidden state sequences
- We could compute each component.
- P({H,H}|{F,F}) =  P(H|F)P(H|F) (they are independent)
- P({F,F}) = P(F|F)P(F)  (Markov property, transition)
---

## 3 Computational applications of HMMs
* Decoding problem (aka uncovering, parsing, or inference)

Given an HMM $M=(A,B,a_0)$, and an observation sequence $O$, find the sequence of states most likely to have produced $O$. <!-- .element : class="fragment" data-fragment-index="1" -->

* Likelihood problem (aka evaluation, or scoring)

Given an HMM $M=(A,B,a_0)$, and an observation sequence $O$, calculate likelihood $P(O|M)$. <!-- .element : class="fragment" data-fragment-index="2" -->


* Learning problem (aka parameter estimation, or fitting)

Given an HMM structure and observation sequence $O$, determine HMM parameters that best fit the training data. <!-- .element : class="fragment" data-fragment-index="3" -->


---

## Solutions to 3 applications of HMMs

* Decoding problem (aka uncovering, parsing, or inference)

Viterbi algorithm <!-- .element : class="fragment" data-fragment-index="1" -->

* Likelihood problem (aka evaluation, or scoring)

Forward-backward algorithm <!-- .element : class="fragment" data-fragment-index="2" -->

* Learning problem (aka parameter estimation, or fitting)

Baum-Welch algorithm <!-- .element : class="fragment" data-fragment-index="3" -->

---

## Decoding problem 

Given HMM $M=(A,B,a_0)$ and observation sequence $O$, find the sequence of states most likely to produce $O$.

![decoding](images/hmm/decoding1.png)


---

## Decoding problem 
![decoding](images/hmm/decoding1.png)<!-- .element width="80%" height="80%" -->

$P(O, S)$: the probability that HMM follows sequence of states $S$ and emits string $O$.

$$P(O,S) = P(O_{1:T}, S_{1:T}) = P(O_{1:T}|S_{1:T}) P(S_{1:T})$$
$P(O_{1:T}|S_{1:T})$: product of emission probabilities,  
$P(S_{1:T})$: product of transition probabilities

---

## Decoding problem

Find $S_{1:T}$ that maximises $P(O_{1:T}|S_{1:T})$ over all possible paths in the HMM

![decoding](images/hmm/decoding1.png)

$N$ states (here: 3, $v_1$, $v_2$, $v_3$); $T$ time steps (here: 6)

That means $N^T$ paths (here: 729)... how do we choose?

---

## Viterbi algorithm

Two steps using dynamic programming
  1. Forward pass (fill matrix)
  2. Backtrack (read back the final sequence)

Filing the matrix: 
* Matrix $v$: $N$ rows (n states), $T$ columns (length of sequence)
* Initialization: $v_0(i) = a_{0i} B(O_1|S_i) = a_{0i} e_l(x_i) $
* Recursion: $ v_l(i) = e_l(x_i) \times max_k(v_k(i-1)a_{kl}) $
* Store a pointer to the path you chose.

Read as: the viterbi score at time $l$ for state $i$ is the emission probability for observation $x_i$ at state $l$ times the (best of the previous scores times transition probability)

---

## Viterbi algorithm
![hidden_markov](images/hmm/hiddenmarkov.svg) <!-- .element width="50%" height="50%" -->

Observations : HHTTTTTTTH, $a_0=(0.5,0.5)$

---

## Viterbi algorithm

Use log space to avoid numerical underflow

![hidden_markov](images/hmm/hiddenmarkovlog.svg) <!-- .element width="40%" height="40%" -->

Observations : HHTTTTTTTH, $a_0=c(-0.69,-0.69)$

![dp](images/hmm/dp1.svg) <!-- .element : class="fragment" data-fragment-index="1" -->

---

## Viterbi algorithm
![hidden_markov](images/hmm/hiddenmarkovlog.svg) <!-- .element width="50%" height="50%" -->

Observations : HHTTTTTTTH, $a_0=c(-0.69,-0.69)$

![dp](images/hmm/dp2.svg) 

Note: log(state prior * state emission) -0.69 - 0.69

---

## Viterbi algorithm
![hidden_markov](images/hmm/hiddenmarkovlog.svg) <!-- .element width="50%" height="50%" -->

Observations : HHTTTTTTTH, $a_0=c(-0.69,-0.69)$

![dp](images/hmm/dp3.svg) 

Note: log(state prior * state emission). -1.2 - 0.69

---

## Viterbi algorithm
![hidden_markov](images/hmm/hiddenmarkovlog.svg) <!-- .element width="50%" height="50%" -->

![dp](images/hmm/dp4.svg) 

Note: -1.39 - 0.36 - .69 = -2.44, -1.9 - 0.92 - 0.69 = -3.51 

---

## Viterbi algorithm
![hidden_markov](images/hmm/hiddenmarkovlog.svg) <!-- .element width="50%" height="50%" -->

![dp](images/hmm/dp5.svg) 

Note: -1.39 - 1.20 - 1.20 = -3.79, -1.9 - 0.51 - 1.20 = -3.60 

---

## Viterbi algorithm
![hidden_markov](images/hmm/hiddenmarkovlog.svg) <!-- .element width="50%" height="50%" -->

![dp](images/hmm/dp6.svg) 

---

## Viterbi algorithm

Two steps using dynamic programming
  1. Forward pass (fill matrix)
  2. Backtrack (read back the final sequence)

Backtracking: 
* Start at the most probable final state
* Use stored pointer to determine how you got there
* Repeat until the beginning

---

## Viterbi algorithm
![hidden_markov](images/hmm/hiddenmarkovlog.svg) <!-- .element width="50%" height="50%" -->

![dp](images/hmm/dp7.svg) 

---

## Viterbi algorithm
![hidden_markov](images/hmm/hiddenmarkovlog.svg) <!-- .element width="50%" height="50%" -->

![dp](images/hmm/dp8.svg) 

---

## Viterbi algorithm
![hidden_markov](images/hmm/hiddenmarkovlog.svg) <!-- .element width="50%" height="50%" -->

![dp](images/hmm/dp9.svg) 

Note:
- Look closely at time steps 3 and 4, where the chosen state doesn't match the higher probability. This is a good example of where the **Future-Aware Decision Making** principle is important. If you weren't future-aware, you would make a different decision here.

---

## Likelihood problem
Given the HMM $M=(A,B,a_0)$, and an observation sequence $O$, calculate likelihood $P(O|M)$.

If we know the states, we can simply multiply probabilities of each observation: 

$$P(O|S) = \prod_{i=1}^T P(O_i|S_i)$$  <!-- .element : class="fragment" data-fragment-index="1" -->

$$ = \prod_{i=1}^T e_S(o_i)$$  <!-- .element : class="fragment" data-fragment-index="1" -->


But we do not know the state sequence! <!-- .element : class="fragment" data-fragment-index="2" -->

---

## Likelihood problem

So, we must sum over all possible state sequences.

$$\begin{eqnarray}
P(O) &=& \sum_{S} P(O,S) \\\\\\
&=& \sum_{S} P(O|S) P(S)
\end{eqnarray}$$

Note: we can compute the total probability of the observations just by summing over all possible hidden state sequences. Again, for M states and T time steps, we are talking M^T paths

---

## Forward-backward algorithm

* Again, dynamic programming
* $N$ rows (number of states), $T$ columns (length of sequence)
* Initialization: $f_0(i) = a_{0i} B(O_1|S_i) = a_{0i} e_l(x_i) $
* Recursion: $ f_l(i) = e_l(x_i) \times \sum_{k}{f_k(i-1)a_{kl}} $

![trellis](images/hmm/decoding.png)

<!-- Initialization: $\alpha_1(i) = a_{0i} B(O_1|S_i)$

Recursion: $\alpha_{t+1}(i) = \sum_i \alpha_t(i) a_{ij} B(O_{t+1}|S_i)$

$$P(O|M) = \sum_{i=1}^M \alpha_{T}(i)$$
 -->

---
# Forward vs viterbi

The only difference between Forward and Viterbi is  
you sum over all the possible paths instead of 
choosing the maximum:


Viterbi recursion step:

$ v_l(i) = e_l(x_i) \times max_k(v_k(i-1)a_{kl}) $

Forward algorithm recursion step:

$ f_l(i) = e_l(x_i) \times \sum_{k}{f_k(i-1)a_{kl}} $


---

### Forward-Backward algorithm: both directions

Forward: Probability of the state for observations now and preceding  
Backward: Probability of the state for following observations 

Then: The two results combined give the distribution over states at any specific point, given all observations in both directions.

The probability of being in a given state at a particular point is $P(\pi_i = k|x) = \frac{f_k(i)b_k(i)}{P(x)}$  <!-- .element : class="fragment" data-fragment-index="1" -->



Note:
- Because we are interested in comparison of states at each time point, we have to scale it. So we will calculate alpha-3F * beta3F / sum(alpha_iF*beta_iF). So when using this method, we also get some confidence values associated with our determination of the most likely state
- $\pi_i$​: The hidden state at time step $k$, is state $i$.
- $x$: The full observation sequence $(x_1,...,x_T)$.


---

## Learning problem

Given observation sequence $O$, and general structure of HMM, determine HMM parameters that best fit the training data. 

### Learning with annotated training data <!-- .element : class="fragment" data-fragment-index="1" -->

If we had training data: A sequence of observations, with a known state sequence <!-- .element : class="fragment" data-fragment-index="1" -->

We could just use maximum likelihood. We could compute: <!-- .element : class="fragment" data-fragment-index="1" -->
- emission probability for each state <!-- .element : class="fragment" data-fragment-index="1" -->
- transition probabilities for each state  <!-- .element : class="fragment" data-fragment-index="1" -->
- initial state probabilities <!-- .element : class="fragment" data-fragment-index="1" -->

We have defined the model. <!-- .element : class="fragment" data-fragment-index="1" -->

---

## Estimating parameters for coin flips
![visible](images/hmm/baum1.svg)

---

## Estimating parameters for coin flips
![visible](images/hmm/baum2.svg)

Emissions: $P(H|F), P(T|F), P(H|L), P(T|L)$ <!-- .element : class="fragment" data-fragment-index="1" -->

Transitions: $P(F|F), P(F|L), P(L|F), P(L|L)$ <!-- .element : class="fragment" data-fragment-index="2" -->

Problem: What if we don't have known state sequences? <!-- .element : class="fragment" data-fragment-index="3" -->

Note: Consider a fully visible Markov model. This would easily allow us to compute the HMM parameters just by maximum likelihood estimation from the training data. For a real HMM, we cannot compute these counts directly from an observation sequence since we don't know which path of states was taken through the machine for a given input. The Baum-Welch algorithm solves this by iteratively estimating the counts. We will start with an estimate for the transition and observation probabilities and then use these estimated probabilities and use the forward-backward procedure to determine the probability of the states at each observation. Then we can use that to determine better estimates of the transition and emission probabilities. 

---

### Learning *without* annotated training data

What algorithm have we seen that can be used to compute maximum likelihood estimates with incomplete data?

* Input is only observations: $o_1, o_2, \ldots, o_T$
* Missing: The hidden state sequence

---

### Learning *without* annotated training data

* Baum-Welch algorithm (EM algorithm)
* Iteratively estimates the missing data and maximizes parameters
* guaranteed to converge to a local optimum
* not guaranteed to be a global optimum

---
### Baum-Welch

Initialize parameters.

E-step (expectation): Use forward-backward to estimate state probabilities

M-stem (maximization): Adjust transition/emission probabilities in the model according to those estimated state probabilities.

Iterate until convergence

---

## Advantages and limitations

* Modularity: HMMs can be combined into larger HMMs
* Transparency: Based on a state model making it interpretable
* Prior knowledge: can be incorporated in the model

* Need accurate, applicable, and sufficiently sized training sets of data
* Model may not converge to a true optimum
* Can be slow in comparison to other methods

---
## Applications

1. ChromHMM: chromatin state segmentation
2. HMMer: protein homology search
3. Universe-HMM: consensus interval sets

---

## ChromHMM

> ChromHMM is a Java program for learning and characterizing chromatin states using a multivariate Hidden Markov Model that models the combinatorial and spatial patterns in data from multiple chromatin marks.

---
ChromHMM Input is a binary matrix of chromatin mark presence/absence across genomic tiles.


- Columns are chromatin marks  
- Rows are genomic tiles

Example:

```console
Cell chr1
Mark1 Mark2 Mark3
0 0 0
0 1 0
0 0 1
```

---

ChromHMM output is a segmentation that divides the genome into chromatin states.

Example

![example](images/hmm/segments.png)

---

## HMMEr

> HMMER is a software package that provides tools for making
probabilistic models of protein and DNA sequence domain families –
called profile hidden Markov models, profile HMMs, or just profiles
– and for using these profiles to annotate new sequences, to search
sequence databases for additional homologs, and to make deep multiple sequence alignments

---

HMMer input is a protein multiple-sequence alignment

Example:

![example](images/hmm/hmmer-globins-sto.png)

---

HMMer learns an HMM model from the input

```
hmmbuild globins4.hmm globins4.sto
```
![example](images/hmm/hmmer-model.png)

---

Trained HMMer models can be used  
to search sequence databases  
for other sequences that match the alignment.

---

## Consensus region set  HMM

> In our model, there are three observed sequences: the number of starts, overlaps, and ends at a given position. The hidden variable corresponds to the different parts of the flexible segment. We can tune transition probabilities, which can be chosen in a way that will prevent unnecessary segmentation, and emission matrix, which describes the relationship between observations and hidden states.  

---

Region set HMM input is a collection of region sets,  
which is transformed into counts of starts, overlap, and ends

![example](images/hmm/universe-regions.svg) <!-- .element style="background:white" -->

Output is a unified segmentation, a consensus region set

---

Thanks for listening
